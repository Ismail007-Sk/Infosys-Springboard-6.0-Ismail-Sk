{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37a932d-ed16-4a5b-9c9e-bcdcacc23194",
   "metadata": {},
   "source": [
    "# NLP Text Preprocessing Notebook\n",
    "This notebook covers:\n",
    "- Text Cleaning\n",
    "- Tokenization\n",
    "- Stopwords\n",
    "- Stemming & Lemmatization\n",
    "- N-grams\n",
    "- Basic Frequency Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ae8704-ab1d-4124-9283-20758b20e4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64867c5-83cf-44c0-aff5-2c17c8752283",
   "metadata": {},
   "source": [
    "- [^ ... ] → remove anything NOT inside the brackets\n",
    "\n",
    "- a-zA-Z → keep only alphabets\n",
    "\n",
    "- \\s → keep spaces\n",
    "  \n",
    "Everything else gets removed.So special characters, numbers, emojis, symbols get deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4f1308-1678-4033-ac66-656aa3f6351f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello this is nlp class  learn text preprocessing google meet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello!!! This is NLP Class 2025. Learn Text Preprocessing @Google Meet. :)\"\n",
    "cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "cleaned_text = cleaned_text.lower().strip()\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a131aea-d948-41c9-bf81-627c4260aa22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'this',\n",
       " 'is',\n",
       " 'nlp',\n",
       " 'class',\n",
       " 'learn',\n",
       " 'text',\n",
       " 'preprocessing',\n",
       " 'google',\n",
       " 'meet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(cleaned_text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c5c556-5063-495f-b743-a09ec6c2d60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'nlp', 'class', 'learn', 'text', 'preprocessing', 'google', 'meet']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb0f0f1-f50d-41c1-85b8-258f6f11342c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'nlp', 'class', 'learn', 'text', 'preprocess', 'googl', 'meet']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed = [ps.stem(word) for word in filtered_tokens]\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2c727c-4a8a-49b0-aa2e-7befb207c209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'nlp', 'class', 'learn', 'text', 'preprocessing', 'google', 'meet']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a77756-ac00-4f32-9271-e6384e0df77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('hello', 'this'),\n",
       "  ('this', 'is'),\n",
       "  ('is', 'nlp'),\n",
       "  ('nlp', 'class'),\n",
       "  ('class', 'learn'),\n",
       "  ('learn', 'text'),\n",
       "  ('text', 'preprocessing'),\n",
       "  ('preprocessing', 'google'),\n",
       "  ('google', 'meet')],\n",
       " [('hello', 'this', 'is'),\n",
       "  ('this', 'is', 'nlp'),\n",
       "  ('is', 'nlp', 'class'),\n",
       "  ('nlp', 'class', 'learn'),\n",
       "  ('class', 'learn', 'text'),\n",
       "  ('learn', 'text', 'preprocessing'),\n",
       "  ('text', 'preprocessing', 'google'),\n",
       "  ('preprocessing', 'google', 'meet')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = list(ngrams(tokens, 2))\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd89bb5-073b-4d4a-87d8-3b8ce1516d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hello': 1,\n",
       "         'this': 1,\n",
       "         'is': 1,\n",
       "         'nlp': 1,\n",
       "         'class': 1,\n",
       "         'learn': 1,\n",
       "         'text': 1,\n",
       "         'preprocessing': 1,\n",
       "         'google': 1,\n",
       "         'meet': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "074ece84-bd15-4cf4-9d1e-5b8030858525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp39-cp39-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\desktop\\anaconda3\\envs\\py310\\lib\\site-packages (from gensim) (1.23.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\desktop\\anaconda3\\envs\\py310\\lib\\site-packages (from gensim) (1.9.3)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\desktop\\anaconda3\\envs\\py310\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.3)\n",
      "Downloading gensim-4.4.0-cp39-cp39-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/24.4 MB 5.6 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.4 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.1/24.4 MB 4.1 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 3.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 3.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.2/24.4 MB 3.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 4.7/24.4 MB 3.1 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.2/24.4 MB 3.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 6.0/24.4 MB 3.0 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 3.1 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.1/24.4 MB 3.0 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 7.6/24.4 MB 2.9 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 8.4/24.4 MB 3.0 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 8.9/24.4 MB 3.0 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 9.4/24.4 MB 3.0 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 3.0 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 11.0/24.4 MB 3.0 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 11.5/24.4 MB 3.0 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 12.6/24.4 MB 3.1 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 13.1/24.4 MB 3.1 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 14.7/24.4 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 15.2/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.7/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.8/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.3/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.4 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.7/24.4 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.4 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.4 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.4 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.4 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.4 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.4 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.2/24.4 MB 2.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.0/24.4 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.8/24.4 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 2.7 MB/s  0:00:08\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Installing collected packages: smart_open, gensim\n",
      "\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   ---------------------------------------- 2/2 [gensim]\n",
      "\n",
      "Successfully installed gensim-4.4.0 smart_open-7.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34ba93f2-27ea-4326-846b-7b4315359b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c208d0-3cbe-4e97-a65e-e93049ce196a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love this product, it is amazing and works g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible service, I will never buy again</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Really enjoyed the experience, very satisfied</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The product broke within days, worst purchase ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delicious food and quick delivery</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Not worth the money, extremely disappointed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I love this product, it is amazing and works g...      1\n",
       "1           Terrible service, I will never buy again      0\n",
       "2      Really enjoyed the experience, very satisfied      1\n",
       "3  The product broke within days, worst purchase ...      0\n",
       "4                  Delicious food and quick delivery      1\n",
       "5        Not worth the money, extremely disappointed      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports & sample data =====\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Optional for Word2Vec\n",
    "# pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Optional for sentence embeddings\n",
    "# pip install sentence-transformers\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For saving\n",
    "import joblib\n",
    "\n",
    "# Sample small dataset (binary sentiment)\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"I love this product, it is amazing and works great\",\n",
    "        \"Terrible service, I will never buy again\",\n",
    "        \"Really enjoyed the experience, very satisfied\",\n",
    "        \"The product broke within days, worst purchase ever\",\n",
    "        \"Delicious food and quick delivery\",\n",
    "        \"Not worth the money, extremely disappointed\"\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 0, 1, 0]  # 1 -> positive, 0 -> negative\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ffe3456-2486-4921-8811-63e8a7ef1fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW features shape: (6, 36)\n",
      "BoW feature names: ['again' 'amazing' 'and' 'broke' 'buy' 'days' 'delicious' 'delivery'\n",
      " 'disappointed' 'enjoyed' 'ever' 'experience' 'extremely' 'food' 'great'\n",
      " 'is' 'it' 'love' 'money' 'never' 'not' 'product' 'purchase' 'quick'\n",
      " 'really' 'satisfied' 'service' 'terrible' 'the' 'this' 'very' 'will'\n",
      " 'within' 'works' 'worst' 'worth']\n",
      "TF-IDF features shape: (6, 70)\n",
      "Example TF-IDF feature names (first 20): ['again' 'amazing' 'amazing and' 'and' 'and quick' 'and works' 'broke'\n",
      " 'broke within' 'buy' 'buy again' 'days' 'days worst' 'delicious'\n",
      " 'delicious food' 'delivery' 'disappointed' 'enjoyed' 'enjoyed the' 'ever'\n",
      " 'experience']\n"
     ]
    }
   ],
   "source": [
    "# BoW and TF-IDF examples =====\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "# Bag of Words\n",
    "cv = CountVectorizer(ngram_range=(1,1), min_df=1)\n",
    "X_bow = cv.fit_transform(texts)\n",
    "print(\"BoW features shape:\", X_bow.shape)\n",
    "print(\"BoW feature names:\", cv.get_feature_names_out())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=1)  # unigrams + bigrams\n",
    "X_tfidf = tfidf.fit_transform(texts)\n",
    "print(\"TF-IDF features shape:\", X_tfidf.shape)\n",
    "print(\"Example TF-IDF feature names (first 20):\", tfidf.get_feature_names_out()[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a6d31bc-7178-46ae-be3b-999da5938172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'product' (shape): (50,)\n",
      "Sentence embeddings shape: (6, 50)\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec demo =====\n",
    "# Tokenize sentences simply (after your preprocessing pipeline)\n",
    "tokenized = [t.lower().split() for t in texts]\n",
    "w2v_model = Word2Vec(sentences=tokenized, vector_size=50, window=5, min_count=1, workers=1, seed=42)\n",
    "\n",
    "# Get embedding for a word\n",
    "print(\"Vector for 'product' (shape):\", w2v_model.wv['product'].shape)\n",
    "\n",
    "# To get sentence embedding: average word vectors (simple)\n",
    "def sentence_vector(sentence, model):\n",
    "    toks = sentence.lower().split()\n",
    "    vecs = [model.wv[w] for w in toks if w in model.wv]\n",
    "    if len(vecs)==0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "sent_emb = np.vstack([sentence_vector(s, w2v_model) for s in texts])\n",
    "print(\"Sentence embeddings shape:\", sent_emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10477c98-bf8d-4efa-bad1-a383970e4fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4, 70) Test: (2, 70)\n"
     ]
    }
   ],
   "source": [
    "# Train-test split (use TF-IDF features) =====\n",
    "X = X_tfidf\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bc7f53c-7d0c-4e77-a489-53a504bf40d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes -> Accuracy: 0.500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "LogisticRegression -> Accuracy: 0.500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "LinearSVC -> Accuracy: 0.500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train baseline models =====\n",
    "models = {\n",
    "    \"NaiveBayes\": MultinomialNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, solver='liblinear'),\n",
    "    \"LinearSVC\": LinearSVC(max_iter=10000)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"{name} -> Accuracy: {acc:.3f}\")\n",
    "    print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ddd74b6-d098-41e0-aa82-bb6d5673816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Precision: 0.5 Recall: 1.0 F1: 0.6666666666666666\n",
      "ROC-AUC: 1.0\n",
      "Confusion Matrix:\n",
      " [[0 1]\n",
      " [0 1]]\n",
      "---- Detailed report ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluation utilities =====\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        probs = model.predict_proba(X_test)[:,1]\n",
    "    else:\n",
    "        # fallback: use decision_function if available and scale to [0,1]\n",
    "        if hasattr(model, \"decision_function\"):\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            scores = model.decision_function(X_test).reshape(-1,1)\n",
    "            probs = MinMaxScaler().fit_transform(scores).ravel()\n",
    "        else:\n",
    "            probs = None\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_test, preds, average='binary', zero_division=0)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Precision:\", p, \"Recall:\", r, \"F1:\", f1)\n",
    "    if probs is not None:\n",
    "        try:\n",
    "            print(\"ROC-AUC:\", roc_auc_score(y_test, probs))\n",
    "        except:\n",
    "            pass\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))\n",
    "    print(\"---- Detailed report ----\")\n",
    "    print(classification_report(y_test, preds))\n",
    "\n",
    "# Example run on the logistic regression model\n",
    "evaluate_model(models['LogisticRegression'], X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6dc1aee-4715-4cbf-a161-5deeeb781b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__C': 0.1, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 1)}\n",
      "Best CV score: 0.4444444444444444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Desktop\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Pipeline + GridSearch =====\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(solver='liblinear', max_iter=1000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__min_df': [1, 2],\n",
    "    'clf__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "gs.fit(df['text'], df['label'])\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "print(\"Best CV score:\", gs.best_score_)\n",
    "\n",
    "# Evaluate best model on holdout (we'll split again)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(df['text'], df['label'], test_size=0.33, random_state=42, stratify=df['label'])\n",
    "best_model = gs.best_estimator_\n",
    "best_model.fit(X_train2, y_train2)\n",
    "preds = best_model.predict(X_test2)\n",
    "print(classification_report(y_test2, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8bfe5-f04e-4290-a107-4ef923fcb4ba",
   "metadata": {},
   "source": [
    "Try ngram_range=(1,2) or (1,3) for capturing phrases.\n",
    "\n",
    "Use stopword removal when vocabulary is noisy.\n",
    "\n",
    "Use character n-grams for short text (reviews, tweets).\n",
    "\n",
    "Limit max_features or use min_df to control vocabulary size.\n",
    "\n",
    "HashingVectorizer for memory-efficient transform on large corpora.\n",
    "\n",
    "Combine TF-IDF with pretrained sentence embeddings for hybrid features (concatenate dense + sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "778d2915-f77e-41d0-913a-54b50b4000ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "# =====  Save & load pipeline =====\n",
    "# Example: save best_model from GridSearch\n",
    "joblib.dump(best_model, \"best_text_pipeline.joblib\")\n",
    "# Load\n",
    "loaded = joblib.load(\"best_text_pipeline.joblib\")\n",
    "# Inference\n",
    "sample = [\"I really hate the support and the product quality\"]\n",
    "print(\"Prediction:\", loaded.predict(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2363e9e-d7a9-44ad-94f0-3ff65903ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install sentence-transformers\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# s_model = SentenceTransformer('all-MiniLM-L6-v2')  # example model\n",
    "# sentence_embeddings = s_model.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "# print(\"Embeddings shape:\", sentence_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6baccf-01e0-4f0f-b035-7c036824ac27",
   "metadata": {},
   "source": [
    "Inference pipeline (production notes) (cell: markdown)\n",
    "\n",
    "Pipeline should include preprocessing → vectorization → model. Save it as a single Pipeline object (as above).\n",
    "\n",
    "Ensure same text cleaning/normalization used at train time is applied during inference.\n",
    "\n",
    "For scaling to production: wrap pipeline in a small API (FastAPI/Flask) and serve with Gunicorn/Uvicorn + container (Docker).\n",
    "\n",
    "If model needs to be retrained periodically, automate dataset collection, validation, and CI tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f5f3d7-8fa6-43b6-9a44-9a051891c05b",
   "metadata": {},
   "source": [
    "**Short assignment / exercises t**\n",
    "Compare performances: CountVectorizer vs TfidfVectorizer vs Word2Vec averaged embeddings on a 10k-sample dataset.\n",
    "\n",
    "Try ngram_range=(1,3) and observe overfitting/feature explosion.\n",
    "\n",
    "Use GridSearchCV to tune C for Logistic Regression and alpha for MultinomialNB.\n",
    "\n",
    "Create an inference API using FastAPI that loads best_text_pipeline.joblib and exposes POST /predict.\n",
    "\n",
    "(Advanced) Fine-tune a small transformer (e.g., DistilBERT) for sentiment classification using Hugging Face **transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004f1cd-9db6-4478-bb33-72d28c2164aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
